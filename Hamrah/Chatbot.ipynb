{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYrP2_y12bhP"
   },
   "source": [
    "**برای اجرای این نوتبوک فایل داده ها باید در کنار این نوتبوک قرار داده بشه**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLbNRl3uOtKH",
    "outputId": "e978d51d-4186-463a-fb7e-8bd81223d607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 170 kB 12.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 64 kB 2.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 235 kB 34.4 MB/s \n",
      "\u001b[?25h  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "pip install -q clean-text[gpl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nPxQgc2bOOtq",
    "outputId": "4066410d-af73-47e5-dbc0-6430831712b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hazm\n",
      "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |█                               | 10 kB 24.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 20 kB 29.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 30 kB 21.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 40 kB 16.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 51 kB 14.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 61 kB 12.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 71 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 81 kB 13.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 92 kB 11.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 102 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 112 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 122 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 133 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 143 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 153 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 163 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 174 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 184 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 194 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 204 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 215 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 225 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 235 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 245 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 256 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 266 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 276 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 286 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 296 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 307 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 316 kB 12.2 MB/s \n",
      "\u001b[?25hCollecting libwapiti>=0.2.1\n",
      "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
      "\u001b[K     |████████████████████████████████| 233 kB 45.1 MB/s \n",
      "\u001b[?25hCollecting nltk==3.3\n",
      "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 48.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
      "Building wheels for collected packages: nltk, libwapiti\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394485 sha256=cd4d9cf02665f1d41dec16b3e1c96725f7e72ed484170efbf14b50afd2192a7f\n",
      "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
      "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154814 sha256=d2e688fe500305e340f1f8d45fcb1602c82417acb0f0c50a95e97255163d4e81\n",
      "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
      "Successfully built nltk libwapiti\n",
      "Installing collected packages: nltk, libwapiti, hazm\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhwZl9I8vxKw",
    "outputId": "daf62d6c-23c7-478e-a333-6d3197d5a007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 9.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 29.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 42.7 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub>=0.0.17\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 5.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 19.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1_mkCkIVoby",
    "outputId": "10f74cd9-827d-423d-fdc0-945cb2501d9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IateOh-uOJJM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext.legacy.data as data\n",
    "import hazm\n",
    "import sys\n",
    "from cleantext import clean\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "w3Bn0KdDPG9j"
   },
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def cleaning(text):\n",
    "    text = text.strip()\n",
    "    \n",
    "    # regular cleaning\n",
    "    text = clean(text\n",
    "        ,fix_unicode=True,\n",
    "        to_ascii=False,\n",
    "        lower=True,\n",
    "        no_line_breaks=True,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=True,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"\",\n",
    "        replace_with_email=\"\",\n",
    "        replace_with_phone_number=\"\",\n",
    "        replace_with_number=\"\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"\",\n",
    "    )\n",
    "\n",
    "    # cleaning htmls\n",
    "    text = cleanhtml(text)\n",
    "    \n",
    "    # normalizing\n",
    "    normalizer = hazm.Normalizer()\n",
    "    text = normalizer.normalize(text)\n",
    "    \n",
    "    # removing wierd patterns\n",
    "    wierd_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\u3030\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u2069\"\n",
    "        u\"\\u2066\"\n",
    "        u\"\\u200c\"\n",
    "        u\"\\u2068\"\n",
    "        u\"\\u2067\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = wierd_pattern.sub(r'', text)\n",
    "    \n",
    "    # removing extra spaces, hashtags\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_YfYbl4eO-3j"
   },
   "outputs": [],
   "source": [
    "original = pd.read_csv('dataset.txt', encoding='utf-8', header=None, delimiter ='$', names=['person1','person2'])\n",
    "original['cleaned_person1'] = original['person1'].apply(cleaning)\n",
    "original['cleaned_person2'] = original['person2'].apply(cleaning)\n",
    "original = original[['cleaned_person1', 'cleaned_person2']]\n",
    "original.columns = ['Question', 'Answer']\n",
    "original.to_csv('persian_questions_answers.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wialKwN6OMay"
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return [tok for tok in hazm.word_tokenize(text)]\n",
    "question = data.Field(lower=True, \n",
    "                      init_token=\"<sos>\",\n",
    "                      eos_token=\"<eos>\",\n",
    "                      tokenize=tokenizer)\n",
    "\n",
    "answer = data.Field(lower=True, \n",
    "                    init_token=\"<sos>\", \n",
    "                    eos_token=\"<eos>\", \n",
    "                    tokenize=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Wd1H225AOWNf"
   },
   "outputs": [],
   "source": [
    "datafields = {\"Question\":(\"q\", question), \"Answer\":(\"a\", answer)}\n",
    "dataset = data.TabularDataset(path=\"persian_questions_answers.csv\", format=\"csv\", fields=datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ipiAvOKrfCiL"
   },
   "outputs": [],
   "source": [
    "g_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZuNzkQq1BzJ"
   },
   "source": [
    "**در این قسمت دیکشنری سوال و جوابها ساخته میشه**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "teo6vXDYOcpP"
   },
   "outputs": [],
   "source": [
    "question.build_vocab(dataset, min_freq=1)\n",
    "answer.build_vocab(dataset, min_freq=1)\n",
    "train_data, valid_data = dataset.split(split_ratio=0.75)\n",
    "train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=32,\n",
    "        device=g_device,\n",
    "        sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.q)\n",
    ")\n",
    "valid_iter = data.BucketIterator(\n",
    "        valid_data,\n",
    "        batch_size=32,\n",
    "        device=g_device,\n",
    "        sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.q)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-Y0KgaW1cqN"
   },
   "source": [
    "**در این قسمت معماری مدل ایجاد میشه**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_rZR-wQyOtta"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedding_size, \n",
    "                 src_vocab_size,\n",
    "                 trg_vocab_size,\n",
    "                 src_pad_index,\n",
    "                 num_heads,\n",
    "                 num_encoder_layers,\n",
    "                 num_decoder_layers,\n",
    "                 dense_dim,\n",
    "                 dropout,\n",
    "                 max_len,\n",
    "                ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.transformer = nn.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            dense_dim,\n",
    "            dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_index = src_pad_index\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = src.transpose(0, 1) == self.src_pad_index\n",
    "        return src_mask\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        src_seq_len, N = src.shape\n",
    "        trg_seq_len, N = trg.shape\n",
    "        src_positions = (\n",
    "            torch.arange(0, src_seq_len).unsqueeze(1).expand(src_seq_len, N).to(device=g_device)\n",
    "        )\n",
    "        trg_positions = (\n",
    "            torch.arange(0, trg_seq_len).unsqueeze(1).expand(trg_seq_len, N).to(device=g_device)\n",
    "        )\n",
    "        embed_src = self.dropout((self.src_word_embedding(src) + self.src_position_embedding(src_positions)))\n",
    "        embed_trg = self.dropout((self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions)))\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_len).to(device=g_device)\n",
    "        out = self.transformer(\n",
    "            embed_src,\n",
    "            embed_trg,\n",
    "            src_key_padding_mask = src_padding_mask,\n",
    "            tgt_mask=trg_mask\n",
    "        )\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6cYB-P01lf7"
   },
   "source": [
    "**در این قسمت پارامترهای مدل مقداردهی میشه**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "B4Ind1rSOyVd"
   },
   "outputs": [],
   "source": [
    "num_epochs = 125\n",
    "learning_rate = 3e-4\n",
    "src_vocab_size = len(question.vocab)\n",
    "trg_vocab_size= len(answer.vocab)\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.1\n",
    "max_len = max(max([batch.a.shape[0] for batch in train_iter]), max([batch.q.shape[0] for batch in train_iter]))\n",
    "dense_dim = 64\n",
    "src_pad_index = question.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "net = Transformer(embedding_size,  src_vocab_size, \n",
    "                  trg_vocab_size, src_pad_index, \n",
    "                  num_heads, num_encoder_layers,\n",
    "                  num_decoder_layers, dense_dim, dropout, max_len).to(device=g_device)\n",
    "\n",
    "trg_pad_index = answer.vocab.stoi[\"<pad>\"]\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=trg_pad_index)\n",
    "opt = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    opt, factor=0.1, patience=10, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9sPw93BrO14L"
   },
   "outputs": [],
   "source": [
    "def predict(test_sentence, model):\n",
    "    max_len = 100\n",
    "    # sen = [w for w in tokenizer.tokenize(test_sentence)]\n",
    "    sen = [w for w in hazm.word_tokenize(test_sentence)]\n",
    "    sen.insert(0, question.init_token)\n",
    "    sen.append(question.eos_token)\n",
    "    inp_sen = [question.vocab.stoi[i] for i in sen]\n",
    "    inp_sen = torch.tensor(inp_sen, dtype=torch.long).unsqueeze(1).to(device=g_device)\n",
    "    outputs = [answer.vocab.stoi[\"<sos>\"]]\n",
    "    for i in range(max_len):\n",
    "        trg = torch.tensor(outputs, dtype=torch.long).unsqueeze(1).to(device=g_device)\n",
    "        with torch.no_grad():\n",
    "            output = model(inp_sen, trg)\n",
    "        best_guess = output.argmax(2)[-1, :].item()\n",
    "        outputs.append(best_guess)\n",
    "        if best_guess == answer.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "    pred_sentence = [answer.vocab.itos[i] for i in outputs]\n",
    "    return pred_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsjiVVG-1tlP"
   },
   "source": [
    "**در این قسمت مدل آموزش داده میشه و ارزیابی میشه و بهترین مدلها روی داده های ارزیابی ذخیره میشه**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LU_oKwV9PLYa",
    "outputId": "16366aee-e106-4c1c-9618-02a2253ed40b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Loss 2.6181348860263824\n",
      "<sos> همه من خوشمزه هستند <eos>\n",
      "***** Current best checkpoint is saved. *****\n",
      "Best valid loss: 2.4816747903823853\n",
      "Valid loss: 2.4816747903823853\n",
      "[Epoch 1] Loss 2.0355038017034532\n",
      "<sos> من یک ربات هستم <eos>\n",
      "***** Current best checkpoint is saved. *****\n",
      "Best valid loss: 2.0003129690885544\n",
      "Valid loss: 2.0003129690885544\n",
      "[Epoch 2] Loss 1.6148752853274346\n",
      "<sos> من یک ربات هستم <eos>\n",
      "[Epoch 3] Loss 1.3648272410035134\n",
      "<sos> من یک ربات هستم <eos>\n",
      "***** Current best checkpoint is saved. *****\n",
      "Best valid loss: 1.798896387219429\n",
      "Valid loss: 1.798896387219429\n",
      "[Epoch 4] Loss 1.054251068085432\n",
      "<sos> من یک ربات هستم <eos>\n",
      "***** Current best checkpoint is saved. *****\n",
      "Best valid loss: 1.74906225502491\n",
      "Valid loss: 1.74906225502491\n",
      "[Epoch 5] Loss 0.8270890291780233\n",
      "<sos> من یک ربات هستم <eos>\n",
      "[Epoch 6] Loss 0.6652787502855062\n",
      "<sos> من یک ربات هستم <eos>\n",
      "[Epoch 7] Loss 0.555126441642642\n",
      "<sos> من یک ربات هستم <eos>\n",
      "[Epoch 8] Loss 0.47926580905914307\n",
      "<sos> ایران <eos>\n",
      "***** Current best checkpoint is saved. *****\n",
      "Best valid loss: 1.6124678999185562\n",
      "Valid loss: 1.6124678999185562\n",
      "[Epoch 9] Loss 0.3703170970082283\n",
      "<sos> ایران <eos>\n",
      "[Epoch 10] Loss 0.29780615409836175\n",
      "<sos> ایران <eos>\n",
      "[Epoch 11] Loss 0.2322350164875388\n",
      "<sos> ایران <eos>\n",
      "[Epoch 12] Loss 0.19070546850562095\n",
      "<sos> ایران <eos>\n",
      "[Epoch 13] Loss 0.14953726935200393\n",
      "<sos> ایران <eos>\n",
      "[Epoch 14] Loss 0.12227001008577645\n",
      "<sos> ایران <eos>\n",
      "[Epoch 15] Loss 0.11269858903251588\n",
      "<sos> ایران <eos>\n",
      "[Epoch 16] Loss 0.0757888974621892\n",
      "<sos> ایران <eos>\n",
      "[Epoch 17] Loss 0.0751572703011334\n",
      "<sos> ایران <eos>\n",
      "[Epoch 18] Loss 0.045168316131457685\n",
      "<sos> ایران <eos>\n",
      "[Epoch 19] Loss 0.042722735367715356\n",
      "<sos> ایران <eos>\n",
      "[Epoch 20] Loss 0.03119832493830472\n",
      "<sos> ایران <eos>\n",
      "[Epoch 21] Loss 0.026665768388193102\n",
      "<sos> ایران <eos>\n",
      "[Epoch 22] Loss 0.024054323334712534\n",
      "<sos> ایران <eos>\n",
      "[Epoch 23] Loss 0.021943171694874763\n",
      "<sos> ایران <eos>\n",
      "[Epoch 24] Loss 0.01539675738895312\n",
      "<sos> ایران <eos>\n",
      "[Epoch 25] Loss 0.015621476748492569\n",
      "<sos> ایران <eos>\n",
      "[Epoch 26] Loss 0.013786308118142188\n",
      "<sos> ایران <eos>\n",
      "[Epoch 27] Loss 0.01462949151173234\n",
      "<sos> ایران <eos>\n",
      "[Epoch 28] Loss 0.014336626027943566\n",
      "<sos> ایران <eos>\n",
      "[Epoch 29] Loss 0.01441013072617352\n",
      "<sos> ایران <eos>\n",
      "[Epoch 30] Loss 0.018154874537140132\n",
      "<sos> ایران <eos>\n",
      "[Epoch 31] Loss 0.035905501747038215\n",
      "<sos> ایران <eos>\n",
      "[Epoch 32] Loss 0.019554311205865815\n",
      "<sos> ایران <eos>\n",
      "[Epoch 33] Loss 0.017026358260773124\n",
      "<sos> ایران <eos>\n",
      "[Epoch 34] Loss 0.015574661432765424\n",
      "<sos> ایران <eos>\n",
      "[Epoch 35] Loss 0.01681478936225176\n",
      "<sos> ایران <eos>\n",
      "[Epoch 36] Loss 0.009947798366192728\n",
      "<sos> ایران <eos>\n",
      "[Epoch 37] Loss 0.01115091418614611\n",
      "<sos> ایران <eos>\n",
      "[Epoch 38] Loss 0.007692946988390759\n",
      "<sos> ایران <eos>\n",
      "[Epoch 39] Loss 0.008969966712174936\n",
      "<sos> ایران <eos>\n",
      "[Epoch 40] Loss 0.0076631076517514884\n",
      "<sos> ایران <eos>\n",
      "[Epoch 41] Loss 0.006777399050770327\n",
      "<sos> ایران <eos>\n",
      "[Epoch 42] Loss 0.006783290978637524\n",
      "<sos> ایران <eos>\n",
      "[Epoch 43] Loss 0.007018633338157087\n",
      "<sos> ایران <eos>\n",
      "[Epoch 44] Loss 0.006818109002779238\n",
      "<sos> ایران <eos>\n",
      "[Epoch 45] Loss 0.006025713929557241\n",
      "<sos> ایران <eos>\n",
      "[Epoch 46] Loss 0.006808786850888282\n",
      "<sos> ایران <eos>\n",
      "[Epoch 47] Loss 0.0051668989530298855\n",
      "<sos> ایران <eos>\n",
      "[Epoch 48] Loss 0.0061330670490860936\n",
      "<sos> ایران <eos>\n",
      "[Epoch 49] Loss 0.005733081939979456\n",
      "<sos> ایران <eos>\n",
      "[Epoch 50] Loss 0.0068391457694815475\n",
      "<sos> ایران <eos>\n",
      "[Epoch 51] Loss 0.007105802671867422\n",
      "<sos> ایران <eos>\n",
      "[Epoch 52] Loss 0.006105131737422198\n",
      "<sos> ایران <eos>\n",
      "[Epoch 53] Loss 0.0050274646462639795\n",
      "<sos> ایران <eos>\n",
      "[Epoch 54] Loss 0.006691113332635723\n",
      "<sos> ایران <eos>\n",
      "[Epoch 55] Loss 0.013460566173307598\n",
      "<sos> ایران <eos>\n",
      "[Epoch 56] Loss 0.008884364450932481\n",
      "<sos> ایران <eos>\n",
      "[Epoch 57] Loss 0.009875015451689251\n",
      "<sos> ایران <eos>\n",
      "[Epoch 58] Loss 0.012425452543538995\n",
      "<sos> ایران <eos>\n",
      "[Epoch 59] Loss 0.009629836602834984\n",
      "<sos> ایران <eos>\n",
      "[Epoch 60] Loss 0.008280218398431316\n",
      "<sos> ایران <eos>\n",
      "[Epoch 61] Loss 0.013284984789788724\n",
      "<sos> ایران <eos>\n",
      "[Epoch 62] Loss 0.024000998376868664\n",
      "<sos> ایران <eos>\n",
      "[Epoch 63] Loss 0.03101020282774698\n",
      "<sos> ایران <eos>\n",
      "[Epoch 64] Loss 0.028951011371100323\n",
      "<sos> ایران <eos>\n",
      "Epoch    65: reducing learning rate of group 0 to 3.0000e-05.\n",
      "[Epoch 65] Loss 0.015797492314595728\n",
      "<sos> ایران <eos>\n",
      "[Epoch 66] Loss 0.011337028560228645\n",
      "<sos> ایران <eos>\n",
      "[Epoch 67] Loss 0.00846476260048803\n",
      "<sos> ایران <eos>\n",
      "[Epoch 68] Loss 0.008751734998077155\n",
      "<sos> ایران <eos>\n",
      "[Epoch 69] Loss 0.006906600703950971\n",
      "<sos> ایران <eos>\n",
      "[Epoch 70] Loss 0.00657176595123019\n",
      "<sos> ایران <eos>\n",
      "[Epoch 71] Loss 0.006805510836420581\n",
      "<sos> ایران <eos>\n",
      "[Epoch 72] Loss 0.006582025051466189\n",
      "<sos> ایران <eos>\n",
      "[Epoch 73] Loss 0.006082080909982324\n",
      "<sos> ایران <eos>\n",
      "[Epoch 74] Loss 0.006454556598328054\n",
      "<sos> ایران <eos>\n",
      "[Epoch 75] Loss 0.006415937110432424\n",
      "<sos> ایران <eos>\n",
      "Epoch    76: reducing learning rate of group 0 to 3.0000e-06.\n",
      "[Epoch 76] Loss 0.006056929682381451\n",
      "<sos> ایران <eos>\n",
      "[Epoch 77] Loss 0.006762882985640317\n",
      "<sos> ایران <eos>\n",
      "[Epoch 78] Loss 0.00577477510960307\n",
      "<sos> ایران <eos>\n",
      "[Epoch 79] Loss 0.005777460170793347\n",
      "<sos> ایران <eos>\n",
      "[Epoch 80] Loss 0.006252762654912658\n",
      "<sos> ایران <eos>\n",
      "[Epoch 81] Loss 0.005378137101070024\n",
      "<sos> ایران <eos>\n",
      "[Epoch 82] Loss 0.006297523970715702\n",
      "<sos> ایران <eos>\n",
      "[Epoch 83] Loss 0.006006910774158314\n",
      "<sos> ایران <eos>\n",
      "[Epoch 84] Loss 0.005947805868345313\n",
      "<sos> ایران <eos>\n",
      "[Epoch 85] Loss 0.0059301664703525605\n",
      "<sos> ایران <eos>\n",
      "[Epoch 86] Loss 0.0055123186320997775\n",
      "<sos> ایران <eos>\n",
      "Epoch    87: reducing learning rate of group 0 to 3.0000e-07.\n",
      "[Epoch 87] Loss 0.00578380145598203\n",
      "<sos> ایران <eos>\n",
      "[Epoch 88] Loss 0.00616361943539232\n",
      "<sos> ایران <eos>\n",
      "[Epoch 89] Loss 0.006480034129344859\n",
      "<sos> ایران <eos>\n",
      "[Epoch 90] Loss 0.005979570234194398\n",
      "<sos> ایران <eos>\n",
      "[Epoch 91] Loss 0.006199376584845595\n",
      "<sos> ایران <eos>\n",
      "[Epoch 92] Loss 0.005405754994717427\n",
      "<sos> ایران <eos>\n",
      "[Epoch 93] Loss 0.005521668409346603\n",
      "<sos> ایران <eos>\n",
      "[Epoch 94] Loss 0.0069477723212912675\n",
      "<sos> ایران <eos>\n",
      "[Epoch 95] Loss 0.006471362494630739\n",
      "<sos> ایران <eos>\n",
      "[Epoch 96] Loss 0.0049985529709374536\n",
      "<sos> ایران <eos>\n",
      "[Epoch 97] Loss 0.006066583702340722\n",
      "<sos> ایران <eos>\n",
      "[Epoch 98] Loss 0.005874458118341863\n",
      "<sos> ایران <eos>\n",
      "[Epoch 99] Loss 0.005914589666645043\n",
      "<sos> ایران <eos>\n",
      "[Epoch 100] Loss 0.005593820057401899\n",
      "<sos> ایران <eos>\n",
      "[Epoch 101] Loss 0.0061538116511655975\n",
      "<sos> ایران <eos>\n",
      "[Epoch 102] Loss 0.006131772033404559\n",
      "<sos> ایران <eos>\n",
      "[Epoch 103] Loss 0.005455592222278938\n",
      "<sos> ایران <eos>\n",
      "[Epoch 104] Loss 0.0052235930779716\n",
      "<sos> ایران <eos>\n",
      "[Epoch 105] Loss 0.004751926162862219\n",
      "<sos> ایران <eos>\n",
      "[Epoch 106] Loss 0.0056555057410150765\n",
      "<sos> ایران <eos>\n",
      "[Epoch 107] Loss 0.006067396888101939\n",
      "<sos> ایران <eos>\n",
      "[Epoch 108] Loss 0.005562542518600821\n",
      "<sos> ایران <eos>\n",
      "[Epoch 109] Loss 0.0054901827359572055\n",
      "<sos> ایران <eos>\n",
      "[Epoch 110] Loss 0.005201824777759612\n",
      "<sos> ایران <eos>\n",
      "[Epoch 111] Loss 0.00599233211542014\n",
      "<sos> ایران <eos>\n",
      "[Epoch 112] Loss 0.0057424928643740715\n",
      "<sos> ایران <eos>\n",
      "[Epoch 113] Loss 0.0057116392825264485\n",
      "<sos> ایران <eos>\n",
      "[Epoch 114] Loss 0.006328864512033761\n",
      "<sos> ایران <eos>\n",
      "[Epoch 115] Loss 0.005078421757207252\n",
      "<sos> ایران <eos>\n",
      "[Epoch 116] Loss 0.0054379972192691636\n",
      "<sos> ایران <eos>\n",
      "Epoch   117: reducing learning rate of group 0 to 3.0000e-08.\n",
      "[Epoch 117] Loss 0.005858770917984657\n",
      "<sos> ایران <eos>\n",
      "[Epoch 118] Loss 0.004896006663329899\n",
      "<sos> ایران <eos>\n",
      "[Epoch 119] Loss 0.005776939773932099\n",
      "<sos> ایران <eos>\n",
      "[Epoch 120] Loss 0.005659384635509923\n",
      "<sos> ایران <eos>\n",
      "[Epoch 121] Loss 0.006373523574438877\n",
      "<sos> ایران <eos>\n",
      "[Epoch 122] Loss 0.005779452377464622\n",
      "<sos> ایران <eos>\n",
      "[Epoch 123] Loss 0.005292729948996567\n",
      "<sos> ایران <eos>\n",
      "[Epoch 124] Loss 0.00576517028384842\n",
      "<sos> ایران <eos>\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"کشور سازنده‌ی تو\" \n",
    "best_loss = sys.float_info.max\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for data in train_iter:\n",
    "        net.train()\n",
    "        outs = net(data.q, data.a[:-1, :])\n",
    "        outs = outs.reshape(-1, outs.shape[2])\n",
    "        target = data.a[1:].reshape(-1)\n",
    "        opt.zero_grad()\n",
    "        loss = loss_fn(outs, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    print(f\"[Epoch {epoch}] Loss {mean_loss}\")\n",
    "    net.eval()\n",
    "    pred_sen = predict(test_sentence, net)\n",
    "    print(\" \".join(pred_sen))\n",
    "    valid_losses = []\n",
    "    with torch.no_grad():\n",
    "        for data in valid_iter:\n",
    "            outs = net(data.q, data.a[:-1, :])\n",
    "            outs = outs.reshape(-1, outs.shape[2])\n",
    "            target = data.a[1:].reshape(-1)\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(outs, target)\n",
    "            valid_losses.append(loss.item())\n",
    "        valid_loss = sum(valid_losses) / len(valid_losses)\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss  \n",
    "              \n",
    "            state = {'epoch': epoch, 'best_model': net, 'optimizer': opt}\n",
    "            torch.save(state, 'checkpoint_' + str(epoch) + '.pth.tar')\n",
    "            print(f\"***** Current best checkpoint is saved. *****\")\n",
    "            print(f\"Best valid loss: {best_loss}\")\n",
    "            print(f\"Valid loss: {valid_loss}\")\n",
    "    scheduler.step(mean_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wB4XPPHw4C0u"
   },
   "source": [
    "در این قسمت آدرس مدل از قبل آموزش داده شده تو تابع لود قرار داده میشه و بعد از اون با بات میشه چت کرد "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5xb3dzyoc7I1",
    "outputId": "f4f9d359-bf9a-42f8-f7d9-7bc4ee682c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "شما:خوشی\n",
      "بات: خدا رو شکر خوبم\n",
      "شما:اهل کجایی\n",
      "بات: من یک ایرانی هستم\n",
      "شما:خداحافظ\n",
      "بات: به سلامت برو بذار ما هم به کارمون برسیم\n",
      "شما:توقف\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('checkpoint_8.pth.tar')\n",
    "net = checkpoint['best_model']\n",
    "while True:\n",
    "    inp_sen = input(\"شما:\")\n",
    "    if inp_sen == 'توقف':\n",
    "      break\n",
    "    pred = predict(inp_sen, net)\n",
    "    print('بات: '+\" \".join(pred[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in c:\\users\\ali\\anaconda3\\lib\\site-packages (2021.9.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\ali\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\ali\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\ali\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\ali\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\ali\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = re.search('(?<=مدل )[0-9|۰-۹]+', 'مدل 8۷9 مشکی')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = regex.search('[0-9|۰-۹]+', 'قبل از ۹۸')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    " a = m.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "b = unidecode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'۹۸'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'85'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1385-int(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
