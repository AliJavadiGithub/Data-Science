{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cleantext\n",
      "  Downloading cleantext-1.1.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from cleantext) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk->cleantext) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk->cleantext) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk->cleantext) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk->cleantext) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from click->nltk->cleantext) (0.4.6)\n",
      "Downloading cleantext-1.1.4-py3-none-any.whl (4.9 kB)\n",
      "Installing collected packages: cleantext\n",
      "Successfully installed cleantext-1.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function clean in module cleantext.cleantext:\n",
      "\n",
      "clean(text: str, clean_all: bool = True, extra_spaces: bool = False, stemming: bool = False, stopwords: bool = False, lowercase: bool = False, numbers: bool = False, punct: bool = False, reg: str = '', reg_replace: str = '', stp_lang: str = 'english') -> str\n",
      "    Given a raw string, return cleaned text\n",
      "    \n",
      "    :param text: Input text to clean\n",
      "    :param clean_all: Execute all cleaning operations\n",
      "    :param extra_spaces: Remove extra white spaces\n",
      "    :param stemming: Stem the words\n",
      "    :param stopwords: Remove stop words\n",
      "    :param lowercase: Convert to lowercase\n",
      "    :param numbers: Remove all digits\n",
      "    :param punct: Remove all punctuations\n",
      "    :param reg: Regular expression for removing or replacing\n",
      "    :param reg_replace: Replace the part with regular expression(reg)\n",
      "    :param stp_lang: Language for stop words\n",
      "    :return: Cleaned text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "clean() got an unexpected keyword argument 'no_line_breaks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 84\u001b[0m\n\u001b[0;32m     79\u001b[0m         dialogue \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m utter \u001b[38;5;129;01min\u001b[39;00m dialogue:\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m#             token_list = tokenizer.tokenize(utter.strip().replace(pre_quote, quotes[1]))\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m#             token_list = process_token_list(token_list)\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m#             text = tokenizer.convert_tokens_to_string(token_list)\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m             utter \u001b[38;5;241m=\u001b[39m cleaning(utter)\n\u001b[0;32m     85\u001b[0m             new_dialogue\u001b[38;5;241m.\u001b[39mappend(utter)\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28mprint\u001b[39m(new_dialogue)\n",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m, in \u001b[0;36mcleaning\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# regular cleaning\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m text \u001b[38;5;241m=\u001b[39m clean(text,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# fix_unicode=True,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# to_ascii=False,\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# lower=True,\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     no_line_breaks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     20\u001b[0m     no_urls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m     no_emails\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m     no_phone_numbers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     23\u001b[0m     no_numbers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m     no_digits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     25\u001b[0m     no_currency_symbols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     26\u001b[0m     no_punct\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     27\u001b[0m     replace_with_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     28\u001b[0m     replace_with_email\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m     replace_with_phone_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m     replace_with_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m     replace_with_digit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     32\u001b[0m     replace_with_currency_symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# cleaning htmls\u001b[39;00m\n\u001b[0;32m     36\u001b[0m text \u001b[38;5;241m=\u001b[39m cleanhtml(text)\n",
      "\u001b[1;31mTypeError\u001b[0m: clean() got an unexpected keyword argument 'no_line_breaks'"
     ]
    }
   ],
   "source": [
    "import hazm\n",
    "from cleantext import clean\n",
    "import re\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def cleaning(text):\n",
    "    text = text.strip()\n",
    "    \n",
    "    # regular cleaning\n",
    "    text = clean(text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=False,\n",
    "        lowercase=True,\n",
    "        no_line_breaks=True,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=True,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"\",\n",
    "        replace_with_email=\"\",\n",
    "        replace_with_phone_number=\"\",\n",
    "        replace_with_number=\"\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"\",\n",
    "    )\n",
    "\n",
    "    # cleaning htmls\n",
    "    text = cleanhtml(text)\n",
    "    \n",
    "    # normalizing\n",
    "    normalizer = hazm.Normalizer()\n",
    "    text = normalizer.normalize(text)\n",
    "    \n",
    "    # removing wierd patterns\n",
    "    wierd_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\u3030\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u2069\"\n",
    "        u\"\\u2066\"\n",
    "        # u\"\\u200c\"\n",
    "        u\"\\u2068\"\n",
    "        u\"\\u2067\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = wierd_pattern.sub(r'', text)\n",
    "    \n",
    "    # removing extra spaces, hashtags\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "with open('dataset.txt', 'r', encoding='utf-8') as f: #open the file\n",
    "    total_dialogues = []\n",
    "    for line in f:\n",
    "        new_dialogue = []\n",
    "        dialogue = line.strip().split('$')\n",
    "        for utter in dialogue:\n",
    "#             token_list = tokenizer.tokenize(utter.strip().replace(pre_quote, quotes[1]))\n",
    "#             token_list = process_token_list(token_list)\n",
    "#             text = tokenizer.convert_tokens_to_string(token_list)\n",
    "            utter = cleaning(utter)\n",
    "            new_dialogue.append(utter)\n",
    "        \n",
    "        print(new_dialogue)\n",
    "        total_dialogues.append(new_dialogue)\n",
    "    \n",
    "    train_frac = 0.8\n",
    "    train_utter_num = 0\n",
    "    valid_utter_num = 0\n",
    "    train_dialogues = total_dialogues[:int(len(total_dialogues)*train_frac)]\n",
    "    valid_dialogues = total_dialogues[int(len(total_dialogues)*train_frac):]\n",
    "    \n",
    "    for dialogue in train_dialogues:\n",
    "        train_utter_num += len(dialogue)\n",
    "        \n",
    "    for dialogue in valid_dialogues:\n",
    "        valid_utter_num += len(dialogue)\n",
    "\n",
    "    print(train_utter_num)\n",
    "    print(train_dialogues)\n",
    "    print(valid_dialogues)\n",
    "    print(valid_utter_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(stop_words_path, encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import hazm\n",
    "from cleantext import clean\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset.txt', encoding='utf-8', header=None, delimiter ='$', names=['person1','person2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print data information\n",
    "print('data information')\n",
    "# print(data.info(), '\\n')\n",
    "\n",
    "# print missing values information\n",
    "print('missing values stats')\n",
    "print(data.isnull().sum(), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def cleaning(text):\n",
    "    text = text.strip()\n",
    "    \n",
    "    # regular cleaning\n",
    "    text = clean(text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=False,\n",
    "        lower=True,\n",
    "        no_line_breaks=True,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=True,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"\",\n",
    "        replace_with_email=\"\",\n",
    "        replace_with_phone_number=\"\",\n",
    "        replace_with_number=\"\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"\",\n",
    "    )\n",
    "\n",
    "    # cleaning htmls\n",
    "    text = cleanhtml(text)\n",
    "    \n",
    "    # normalizing\n",
    "    normalizer = hazm.Normalizer()\n",
    "    text = normalizer.normalize(text)\n",
    "    \n",
    "    # removing wierd patterns\n",
    "    wierd_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\u3030\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u2069\"\n",
    "        u\"\\u2066\"\n",
    "        # u\"\\u200c\"\n",
    "        u\"\\u2068\"\n",
    "        u\"\\u2067\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = wierd_pattern.sub(r'', text)\n",
    "    \n",
    "    # removing extra spaces, hashtags\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_person1'] = data['person1'].apply(cleaning)\n",
    "data['cleaned_person2'] = data['person2'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['cleaned_person1', 'cleaned_person2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['person1', 'person2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://cdn-lfs.huggingface.co/m3hrdadfi/gpt2-persian-qa.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
    "\n",
    "# v3.0\n",
    "model_name_or_path = \"HooshvareLab/bert-fa-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelWithLMHead, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer\n",
    "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "# model_name_or_path = \"HooshvareLab/bert-fa-base-uncased\"\n",
    "# config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained('flax-community/gpt2-medium-persian')\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('flax-community/gpt2-medium-persian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = {'bos_token':'[bos]', 'eos_token':'[eos]', 'speaker1_token':'[speaker1]', 'speaker2':'[speaker2]', 'pad_token':'[pad]'}\n",
    "\n",
    "# We can add these special tokens to the vocabulary and the embeddings of the model:\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model_name_or_path = \"m3hrdadfi/gpt2-persian-qa\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
